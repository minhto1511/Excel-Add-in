# ðŸ“š Excel Dataset Collection & Translation Scripts

Scripts Ä‘á»ƒ thu tháº­p vÃ  dá»‹ch datasets Excel cho AI training.

## ðŸš€ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t Python dependencies

```bash
cd scripts
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh API Keys

Táº¡o file `.env` trong thÆ° má»¥c `scripts/`:

```env
OPENAI_API_KEY=sk-your-openai-api-key-here
STACKOVERFLOW_API_KEY=your-stackoverflow-key  # Optional
```

## ðŸ“Š Script 1: Thu tháº­p Datasets

### Cháº¡y script collect_datasets.py

```bash
python collect_datasets.py
```

### Káº¿t quáº£:

Script sáº½ táº¡o cÃ¡c file trong folder `datasets/`:

1. **microsoft_docs_functions.json** - TÃ i liá»‡u 400+ Excel functions tá»« Microsoft
2. **stackoverflow_excel_questions.csv** - Excel questions tá»« StackOverflow (score cao)
3. **synthetic_formulas.csv** - 2000+ synthetic training examples

### TÃ¹y chá»‰nh:

```python
# Thu tháº­p thÃªm StackOverflow pages
collector.collect_stackoverflow(max_pages=50)  # Default: 20

# Táº¡o thÃªm synthetic data
collector.generate_synthetic_data(count=5000)  # Default: 2000
```

## ðŸŒ Script 2: Dá»‹ch Datasets

### Option 1: DÃ¹ng OpenAI (Recommended)

```bash
python translate_dataset.py \
  --input ../datasets/synthetic_formulas.csv \
  --output ../datasets/synthetic_formulas_vi.csv \
  --columns query_en \
  --method openai \
  --api-key YOUR_OPENAI_KEY
```

### Option 2: DÃ¹ng Google Translate (Free)

```bash
python translate_dataset.py \
  --input ../datasets/synthetic_formulas.csv \
  --output ../datasets/synthetic_formulas_vi.csv \
  --columns query_en \
  --method google
```

### Dá»‹ch nhiá»u columns:

```bash
python translate_dataset.py \
  -i ../datasets/stackoverflow_excel_questions.csv \
  -o ../datasets/stackoverflow_excel_questions_vi.csv \
  -c title body \
  -m openai \
  -k YOUR_OPENAI_KEY
```

### TÃ­nh nÄƒng:

âœ… **Protect Excel Functions** - Giá»¯ nguyÃªn tÃªn hÃ m (SUM, VLOOKUP, IF...)  
âœ… **Protect Cell References** - Giá»¯ nguyÃªn A1, B2:D10, $A$1...  
âœ… **Protect Formulas** - Giá»¯ nguyÃªn =SUM(A1:A10)  
âœ… **Natural Translation** - Dá»‹ch context tá»± nhiÃªn  
âœ… **Batch Processing** - Xá»­ lÃ½ hÃ ng loáº¡t vá»›i rate limiting  

## ðŸ“‹ Workflow hoÃ n chá»‰nh

### Step 1: Thu tháº­p datasets

```bash
python collect_datasets.py
```

Káº¿t quáº£:
- `datasets/microsoft_docs_functions.json` (400+ functions)
- `datasets/stackoverflow_excel_questions.csv` (2000+ questions)
- `datasets/synthetic_formulas.csv` (2000 examples)

### Step 2: Dá»‹ch sang tiáº¿ng Viá»‡t

```bash
# Dá»‹ch synthetic data
python translate_dataset.py \
  -i ../datasets/synthetic_formulas.csv \
  -o ../datasets/synthetic_formulas_vi.csv \
  -c query_en \
  -m openai \
  -k YOUR_OPENAI_KEY

# Dá»‹ch StackOverflow data
python translate_dataset.py \
  -i ../datasets/stackoverflow_excel_questions.csv \
  -o ../datasets/stackoverflow_excel_questions_vi.csv \
  -c title body \
  -m openai \
  -k YOUR_OPENAI_KEY
```

### Step 3: Combine datasets

```python
import pandas as pd

# Load all datasets
synthetic = pd.read_csv('../datasets/synthetic_formulas_vi.csv')
stackoverflow = pd.read_csv('../datasets/stackoverflow_excel_questions_vi.csv')
manual = pd.read_json('../datasets/excel_formulas_training_VI.json')

# Standardize columns
synthetic_clean = synthetic[['query_vi', 'formula', 'difficulty']].copy()
synthetic_clean.columns = ['query', 'formula', 'difficulty']

# Combine
combined = pd.concat([
    manual[['query_vi', 'formula']].rename(columns={'query_vi': 'query'}),
    synthetic_clean[['query', 'formula']],
], ignore_index=True)

# Remove duplicates
combined = combined.drop_duplicates(subset=['query'])

# Save
combined.to_csv('../datasets/combined_training_data_vi.csv', index=False)
print(f"Total training examples: {len(combined)}")
```

### Step 4: Format cho Fine-tuning

```python
import json

# Load combined data
df = pd.read_csv('../datasets/combined_training_data_vi.csv')

# Format to OpenAI fine-tuning format
with open('../datasets/training_data_openai.jsonl', 'w', encoding='utf-8') as f:
    for _, row in df.iterrows():
        example = {
            "messages": [
                {
                    "role": "system",
                    "content": "Báº¡n lÃ  chuyÃªn gia Excel. Táº¡o cÃ´ng thá»©c Excel tá»« mÃ´ táº£ báº±ng tiáº¿ng Viá»‡t."
                },
                {
                    "role": "user",
                    "content": row['query']
                },
                {
                    "role": "assistant",
                    "content": json.dumps({
                        "formula": row['formula'],
                        "explanation": row.get('explanation_vi', '')
                    }, ensure_ascii=False)
                }
            ]
        }
        f.write(json.dumps(example, ensure_ascii=False) + '\n')

print("âœ… Training data ready for fine-tuning!")
```

## ðŸ’¡ Tips

### 1. Chi phÃ­ Æ°á»›c tÃ­nh

**OpenAI Translation:**
- GPT-4-turbo: ~$0.01-0.03 per 1K tokens
- Dá»‹ch 5000 queries (avg 20 tokens): ~$2-5

**OpenAI Fine-tuning:**
- Training: $0.008/1K tokens
- Inference: $0.012/1K tokens
- 10,000 examples Ã— 100 tokens Ã— 3 epochs: ~$24

### 2. Quality check

```python
from translate_dataset import ExcelDatasetTranslator

translator = ExcelDatasetTranslator(api_key='your-key', method='openai')

# Test má»™t vÃ i examples
test_texts = [
    "Calculate sum of column A",
    "Find maximum value in range B2:B50",
    "If A1 > 100 then 'High' else 'Low'"
]

for text in test_texts:
    translated = translator.translate_text_openai(text)
    print(f"EN: {text}")
    print(f"VI: {translated}")
    print()
```

### 3. Incremental translation

Náº¿u bá»‹ interrupt, script sáº½ lÆ°u progress. Äá»ƒ continue:

```python
# Load partially translated data
df = pd.read_csv('output_partial.csv')

# Find untranslated rows
untranslated = df[df['query_vi'].isna()]

# Translate only remaining
translator = ExcelDatasetTranslator(api_key='key', method='openai')
df.loc[df['query_vi'].isna(), 'query_vi'] = translator.translate_batch(
    untranslated['query_en'].tolist()
)

# Save
df.to_csv('output_complete.csv', index=False)
```

## ðŸ› Troubleshooting

### Lá»—i: "googletrans not working"

```bash
pip uninstall googletrans
pip install googletrans==3.1.0a0
```

### Lá»—i: "OpenAI rate limit"

ThÃªm sleep time hoáº·c giáº£m batch size:

```python
translator.translate_batch(texts, batch_size=5)  # Slower but safer
```

### Lá»—i: "Module not found"

```bash
pip install -r requirements.txt --upgrade
```

## ðŸ“Š Káº¿t quáº£ mong Ä‘á»£i

Sau khi cháº¡y xong toÃ n bá»™ workflow:

```
datasets/
â”œâ”€â”€ microsoft_docs_functions.json         (400 functions)
â”œâ”€â”€ stackoverflow_excel_questions.csv     (2000+ questions)
â”œâ”€â”€ stackoverflow_excel_questions_vi.csv  (translated)
â”œâ”€â”€ synthetic_formulas.csv                (2000 examples)
â”œâ”€â”€ synthetic_formulas_vi.csv             (translated)
â”œâ”€â”€ excel_formulas_training_EN.json       (50 curated examples)
â”œâ”€â”€ excel_formulas_training_VI.json       (50 curated, Vietnamese)
â”œâ”€â”€ combined_training_data_vi.csv         (5000+ combined)
â””â”€â”€ training_data_openai.jsonl            (ready for fine-tuning)
```

**Total: 5,000-10,000 high-quality Vietnamese examples!**

## ðŸ“ž LiÃªn há»‡ & Há»— trá»£

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check Python version >= 3.8
2. Check API keys valid
3. Check internet connection
4. Read error messages carefully
5. Try with smaller batch size first

Happy training! ðŸš€

